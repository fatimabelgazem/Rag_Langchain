# ğŸ” RAG Chatbot avec Flask, ZenML, Chroma et Ollama

## ğŸ“Œ Description

Ce projet implÃ©mente un chatbot basÃ© sur le paradigme Retrieval-Augmented Generation (RAG) en utilisant Flask, ZenML, ChromaDB et Ollama pour rÃ©pondre aux questions des utilisateurs sur la base de documents PDF chargÃ©s dynamiquement.

## ğŸ—ï¸ Architecture du Projet

Flask : Fournit une API pour interagir avec le chatbot.

ZenML : GÃ¨re le pipeline de traitement des documents et des requÃªtes.

ChromaDB : Base de donnÃ©es vectorielle pour la recherche des passages pertinents.

Ollama : ModÃ¨le LLM pour gÃ©nÃ©rer des rÃ©ponses basÃ©es sur les passages rÃ©cupÃ©rÃ©s.

## ğŸ“¦ Installation

Assurez-vous d'avoir Python installÃ©, puis exÃ©cutez les commandes suivantes :

 1- Cloner le projet
```bash 
git clone <lien_du_repo>
cd <nom_du_repo>

```

 2- Installer les dÃ©pendances
 
```bash

pip install flask zenml langchain langchain-community langchain-chroma chromadb ollama opencv-python PyMuPDF

```
##ğŸš€ Utilisation

DÃ©marrer l'API Flask :

```bash
python app.py
```

AccÃ©der Ã  l'interface :

Ouvrez http://127.0.0.1:5000/ dans votre navigateur.

Envoyer une requÃªte Ã  l'API :

curl -X POST http://127.0.0.1:5000/ask -H "Content-Type: application/json" -d '{"question": "Quelle est la capitale de la France ?"}'

## ğŸ“š FonctionnalitÃ©s du Pipeline RAG

Le pipeline suit les Ã©tapes suivantes :

Chargement des documents (fichiers PDF).

Segmentation des documents en petits morceaux.

Stockage des embeddings dans ChromaDB.

Recherche de passages pertinents via Similarity Search.

GÃ©nÃ©ration de rÃ©ponse avec le modÃ¨le Ollama.

## ğŸ§  ModÃ¨les Ollama

Le projet utilise les modÃ¨les suivants :

nomic-embed-text : GÃ©nÃ¨re des embeddings pour la recherche vectorielle.

llama3 : GÃ©nÃ¨re des rÃ©ponses basÃ©es sur les passages trouvÃ©s.

Vous pouvez tester d'autres modÃ¨les disponibles avec :

from langchain_community.llms.ollama import Ollama
model = Ollama(model="mistral")

## ğŸ› ï¸ Personnalisation

Modifier DATA_PATH pour charger d'autres documents.

Ajuster chunk_size et chunk_overlap dans RecursiveCharacterTextSplitter pour un meilleur dÃ©coupage.

Changer le modÃ¨le Ollama en mettant Ã  jour model="llama3" dans Ollama(model=...).

# ğŸ” RAG Chatbot avec Flask, ZenML, Chroma et Ollama
--- 

## ğŸ“Œ Description

Ce projet implÃ©mente un chatbot basÃ© sur le paradigme Retrieval-Augmented Generation (RAG) en utilisant Flask, ZenML, ChromaDB et Ollama pour rÃ©pondre aux questions des utilisateurs sur la base de documents PDF chargÃ©s dynamiquement.

---

## ğŸ—ï¸ Architecture du Projet

Flask : Fournit une API pour interagir avec le chatbot.

ZenML : GÃ¨re le pipeline de traitement des documents et des requÃªtes.

ChromaDB : Base de donnÃ©es vectorielle pour la recherche des passages pertinents.

Ollama : ModÃ¨le LLM pour gÃ©nÃ©rer des rÃ©ponses basÃ©es sur les passages rÃ©cupÃ©rÃ©s.

---
## ğŸ“¦ Installation

Assurez-vous d'avoir Python installÃ©, puis exÃ©cutez les commandes suivantes :

 1- Cloner le projet
 
```bash 
git clone <lien_du_repo>
cd <nom_du_repo>

```

 2- Installer les dÃ©pendances
 
```bash

pip install flask zenml langchain langchain-community langchain-chroma chromadb ollama opencv-python PyMuPDF

```
---

## ğŸš€ Utilisation

1- DÃ©marrer l'API Flask :

```bash
python app.py
```

2- AccÃ©der Ã  l'interface :

Ouvrez http://127.0.0.1:5000/ dans votre navigateur.

3- Envoyer une requÃªte Ã  l'API :

```bash

curl -X POST http://127.0.0.1:5000/ask -H "Content-Type: application/json" -d '{"question": "Quelle est la capitale de la France ?"}'
```
---

## ğŸ“š FonctionnalitÃ©s du Pipeline RAG

###Le pipeline suit les Ã©tapes suivantes :

1- Chargement des documents (fichiers PDF).

2- Segmentation des documents en petits morceaux.

3- Stockage des embeddings dans ChromaDB.

4- Recherche de passages pertinents via Similarity Search.

5- GÃ©nÃ©ration de rÃ©ponse avec le modÃ¨le Ollama.

---

## ğŸ§  ModÃ¨les Ollama

Le projet utilise les modÃ¨les suivants :

**nomic-embed-text** : GÃ©nÃ¨re des embeddings pour la recherche vectorielle.

**llama3** : GÃ©nÃ¨re des rÃ©ponses basÃ©es sur les passages trouvÃ©s.

Vous pouvez tester d'autres modÃ¨les disponibles avec :

```bash
from langchain_community.llms.ollama import Ollama
model = Ollama(model="mistral")
```
---

## ğŸ› ï¸ Personnalisation

Modifier DATA_PATH pour charger d'autres documents.

Ajuster chunk_size et chunk_overlap dans RecursiveCharacterTextSplitter pour un meilleur dÃ©coupage.

Changer le modÃ¨le Ollama en mettant Ã  jour model="llama3" dans Ollama(model=...).
